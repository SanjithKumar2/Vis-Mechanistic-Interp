{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e0f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\miniconda3\\envs\\xformers-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.fx import symbolic_trace\n",
    "from torch.autograd import Function\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "import timm\n",
    "import json\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76537566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "m = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49dbc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample image\n",
    "url = \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\"\n",
    "image = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    ),\n",
    "])\n",
    "data = transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e50738ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e24c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conservation_check(func):\n",
    "    #TODO: bug in add2_fn\n",
    "    def wrapped(ctx, *out_relevance):\n",
    "\n",
    "        inp_relevance = func(ctx, *out_relevance)\n",
    "\n",
    "        if CONSERVATION_CHECK_FLAG[0]:\n",
    "\n",
    "            out_rel_sum = sum(r.float().sum() for r in out_relevance if r is not None)\n",
    "            inp_elements = sum(r.float().sum() for r in inp_relevance if r is not None)\n",
    "            if CONSERVATION_CHECK_FLAG[1]:\n",
    "                print(func.__name__, out_rel_sum, inp_elements)\n",
    "            # inp_rel_mean = out_rel_sum/inp_elements\n",
    "\n",
    "            # if torch.isnan(inp_rel_mean).any():\n",
    "            #     raise ValueError(f\"NaN at {func}\")\n",
    "            # if REPLACE_WITH_MEAN:\n",
    "            #     inp_relevance = tuple(torch.full(r.shape, inp_rel_mean).to(r.device) if r is not None else None for r in inp_relevance)\n",
    "\n",
    "\n",
    "        return inp_relevance\n",
    "        \n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5c20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class epsilon_lrp_fn:\n",
    "    def __init__(self, fn, epsilon):\n",
    "        self.fn = fn\n",
    "        self.epsilon = epsilon\n",
    "        self.requires_grads = None\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        self.requires_grads = [inp.requires_grad for inp in inputs]\n",
    "        if not any(self.requires_grads):\n",
    "            return self.fn(*inputs)\n",
    "\n",
    "        self.inputs = tuple(inp.detach().requires_grad_() if inp.requires_grad else inp for inp in inputs)\n",
    "        with torch.enable_grad():\n",
    "            self.outputs = self.fn(*self.inputs)\n",
    "        return self.outputs.detach()\n",
    "\n",
    "    @conservation_check\n",
    "    def backward(self, *out_relevance):\n",
    "        inputs = [self.inputs[i] for i, req in enumerate(self.requires_grads) if req]\n",
    "        outputs = self.outputs\n",
    "        relevance_norm = out_relevance[0] / (outputs + self.epsilon)\n",
    "        grads = torch.autograd.grad(outputs, inputs, relevance_norm)\n",
    "        relevance = iter([grads[i].mul_(inputs[i]) for i in range(len(inputs))])\n",
    "        return tuple(next(relevance) if req else None for req in self.requires_grads)\n",
    "\n",
    "\n",
    "class identity_fn:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.output = self.fn(input)\n",
    "        return self.output\n",
    "\n",
    "    @conservation_check\n",
    "    def backward(self, *out_relevance):\n",
    "        return out_relevance\n",
    "\n",
    "\n",
    "class softmax_fn:\n",
    "    def __init__(self, dim, temprature=1.0, dtype=None, inplace=False):\n",
    "        self.dim = dim\n",
    "        self.temprature = temprature\n",
    "        self.dtype = dtype\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.dtype is not None:\n",
    "            inputs = inputs.to(self.dtype)\n",
    "        inputs = inputs / self.temprature\n",
    "        outputs = F.softmax(inputs, dim=self.dim, dtype=self.dtype)\n",
    "        self.inputs, self.outputs = inputs, outputs\n",
    "        return outputs\n",
    "\n",
    "    @conservation_check\n",
    "    def backward(self, *R_out):\n",
    "        R_out = R_out[0]\n",
    "        inputs, outputs = self.inputs, self.outputs\n",
    "        inputs = torch.where(torch.isneginf(inputs), torch.tensor(0).to(inputs), inputs)\n",
    "        if self.inplace:\n",
    "            R_in = (R_out.sub_(outputs.mul_(R_out.sum(-1, keepdim=True)))).mul_(inputs)\n",
    "        else:\n",
    "            R_in = inputs * (R_out - outputs * R_out.sum(-1, keepdim=True))\n",
    "        return R_in\n",
    "\n",
    "\n",
    "class linear_fn:\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs, weight, bias):\n",
    "        self.inputs, self.weight = inputs, weight\n",
    "        self.outputs = F.linear(inputs, weight, bias)\n",
    "        return self.outputs\n",
    "\n",
    "    @conservation_check\n",
    "    def backward(self, *R_out):\n",
    "        R_out = R_out[0]\n",
    "        S = R_out / (self.outputs + self.epsilon)\n",
    "        R_in = torch.matmul(S, self.weight) * self.inputs\n",
    "        return R_in\n",
    "\n",
    "\n",
    "class matmul_fn:\n",
    "    def __init__(self, epsilon=1e-12, inplace=False):\n",
    "        self.epsilon = epsilon\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input_a, input_b):\n",
    "        self.input_a, self.input_b = input_a, input_b\n",
    "        self.output = torch.matmul(input_a, input_b)\n",
    "        return self.output\n",
    "\n",
    "    @conservation_check\n",
    "    def backward(self, *R_out):\n",
    "        R_out = R_out[0]\n",
    "        if self.inplace:\n",
    "            S = R_out.div_(self.output.mul_(2).add_(self.epsilon))\n",
    "        else:\n",
    "            S = R_out / ((2 * self.output) + self.epsilon)\n",
    "        R_ina = torch.matmul(S, self.input_b.T).mul_(self.input_a)\n",
    "        R_inb = torch.matmul(self.input_a.T, S).mul_(self.input_b)\n",
    "        return R_ina, R_inb\n",
    "\n",
    "\n",
    "class add_2_tensors_fn:\n",
    "    def __init__(self, inplace=False, epsilon=1e-8):\n",
    "        self.inplace = inplace\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, input_a, input_b):\n",
    "        self.input_a, self.input_b = input_a, input_b\n",
    "        self.requires_grads = [i for i, inp in enumerate((input_a, input_b))\n",
    "                               if isinstance(inp, torch.Tensor) and inp.requires_grad]\n",
    "        self.outputs = input_a + input_b\n",
    "        return self.outputs\n",
    "\n",
    "    @conservation_check\n",
    "    def backward(self, *R_out):\n",
    "        if not self.requires_grads:\n",
    "            return None, None\n",
    "        R_out = R_out[0]\n",
    "        denom = (self.input_a + self.input_b + self.epsilon)\n",
    "        R_ina = (R_out * self.input_a) / denom\n",
    "        R_inb = (R_out * self.input_b) / denom\n",
    "        return (R_ina if 0 in self.requires_grads else None,\n",
    "                R_inb if 1 in self.requires_grads else None)\n",
    "\n",
    "\n",
    "class mul2_fn:\n",
    "    def __init__(self, inplace=False):\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, input_a, input_b):\n",
    "        self.input_a, self.input_b = input_a, input_b\n",
    "        self.requires_grads = [i for i, inp in enumerate((input_a, input_b))\n",
    "                               if isinstance(inp, torch.Tensor) and inp.requires_grad]\n",
    "        return input_a * input_b\n",
    "\n",
    "    @conservation_check\n",
    "    def backward(self, *R_out):\n",
    "        n = len(self.requires_grads)\n",
    "        R_out = R_out[0]\n",
    "        R_in = R_out.div_(n) if self.inplace else R_out / n\n",
    "        return tuple(R_in if i in self.requires_grads else None for i in range(2))\n",
    "\n",
    "\n",
    "class layernorm_fn:\n",
    "    def __init__(self, epsilon=1e-6, var_epsilon=1e-6):\n",
    "        self.epsilon = epsilon\n",
    "        self.var_epsilon = var_epsilon\n",
    "\n",
    "    def forward(self, input, weight, bias):\n",
    "        with torch.enable_grad():\n",
    "            mean = input.mean(-1, keepdim=True)\n",
    "            var = ((input - mean) ** 2).mean(-1, keepdim=True)\n",
    "            std = (var + self.var_epsilon).sqrt()\n",
    "            y = (input - mean) / std.detach()\n",
    "            y = y * weight + bias\n",
    "            self.input, self.y = input, y\n",
    "        return y.detach()\n",
    "\n",
    "    @conservation_check\n",
    "    def backward(self, *R_out):\n",
    "        R_out = R_out[0]\n",
    "        R_norm = R_out / (self.y + self.epsilon)\n",
    "        grads = torch.autograd.grad(self.y, self.input, R_norm)[0]\n",
    "        R_in = grads * self.input\n",
    "        return R_in\n",
    "\n",
    "\n",
    "class conv_fn:\n",
    "    def __init__(self, lowest=0., highest=1.):\n",
    "        self.lowest = lowest\n",
    "        self.highest = highest\n",
    "\n",
    "    def forward(self, inputs, module):\n",
    "        self.module = module\n",
    "        self.inputs = inputs\n",
    "        self.output = module(inputs)\n",
    "        self.stride, self.padding, self.kernel = module.stride, module.padding, module.kernel_size\n",
    "        self.weight = module.weight\n",
    "        return self.output\n",
    "\n",
    "    @conservation_check\n",
    "    def backward(self, *R_out):\n",
    "        R = R_out[0]\n",
    "        stride, padding, kernel = self.stride, self.padding, self.kernel\n",
    "        activation, Z_O, weight = self.inputs, self.output, self.weight\n",
    "        output_padding = activation.size(2) - ((R.size(2) - 1) * stride[0] - 2 * padding[0] + kernel[0])\n",
    "        W_L = torch.clamp(weight, min=0)\n",
    "        W_H = torch.clamp(weight, max=0)\n",
    "\n",
    "        L = torch.ones_like(activation) * self.lowest\n",
    "        H = torch.ones_like(activation) * self.highest\n",
    "        Z_L = F.conv2d(L, W_L, stride=stride, padding=padding)\n",
    "        Z_H = F.conv2d(H, W_H, stride=stride, padding=padding)\n",
    "        Z = Z_O - Z_L - Z_H + 1e-9\n",
    "        S = R / Z\n",
    "\n",
    "        C_O = F.conv_transpose2d(S, weight, stride=stride, padding=padding, output_padding=output_padding)\n",
    "        C_L = F.conv_transpose2d(S, W_L, stride=stride, padding=padding, output_padding=output_padding)\n",
    "        C_H = F.conv_transpose2d(S, W_H, stride=stride, padding=padding, output_padding=output_padding)\n",
    "        R_in = activation * C_O - L * C_L - H * C_H\n",
    "        return R_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5cf1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(inputs, dim, temprature=1.0, dtype=None, inplace=False, **kwargs):\n",
    "    return softmax_fn(dim=dim, temprature=temprature, dtype=dtype, inplace=inplace).forward(inputs)\n",
    "\n",
    "\n",
    "def linear(fn, inputs, epsilon=1e-6, **kwargs):\n",
    "    return linear_fn(epsilon=epsilon).forward(inputs, fn.weight, fn.bias)\n",
    "\n",
    "\n",
    "def matmul(input_a, input_b, epsilon=1e-12, inplace=False, **kwargs):\n",
    "    return matmul_fn(epsilon=epsilon, inplace=inplace).forward(input_a, input_b)\n",
    "\n",
    "\n",
    "def add(input_a, input_b, inplace=False, epsilon=1e-8, **kwargs):\n",
    "    return add_2_tensors_fn(inplace=inplace, epsilon=epsilon).forward(input_a, input_b)\n",
    "\n",
    "\n",
    "def mul(input_a, input_b, inplace=False, **kwargs):\n",
    "    return mul2_fn(inplace=inplace).forward(input_a, input_b)\n",
    "\n",
    "\n",
    "def layernorm(fn, inputs, epsilon=1e-6, var_epsilon=1e-6, **kwargs):\n",
    "    return layernorm_fn(epsilon=epsilon, var_epsilon=var_epsilon).forward(inputs, fn.weight, fn.bias)\n",
    "\n",
    "\n",
    "def identity(fn, inputs, **kwargs):\n",
    "    return identity_fn(fn).forward(inputs)\n",
    "\n",
    "\n",
    "def epsilon_lrp(fn, epsilon, *inputs, **kwargs):\n",
    "    return epsilon_lrp_fn(fn, epsilon).forward(*inputs)\n",
    "\n",
    "\n",
    "def conv_2d(fn, inputs, **kwargs):\n",
    "    return conv_fn().forward(inputs, fn)\n",
    "\n",
    "def multihead_attn_fn_cp(fn,\n",
    "        x,\n",
    "        num_heads: int = 12,\n",
    "        attn_mask=None,\n",
    "        qk_norm=False,\n",
    "        scale_norm=False, **kwargs):\n",
    "        qkv, q_norm, k_norm, attn_drop, norm, proj, proj_drop, dim = fn.qkv, fn.q_norm, fn.k_norm, fn.attn_drop, fn.norm, fn.proj, fn.proj_drop, int(x.shape[-1])\n",
    "        assert dim%num_heads==0, 'dim should be divisible by num_heads'\n",
    "        if qk_norm or scale_norm:\n",
    "            assert norm_layer is not None, 'norm_layer must be provided if qk_norm or scale_norm is True'\n",
    "        head_dim = dim//num_heads\n",
    "        scale = head_dim ** -0.5\n",
    "        B, N, C = x.shape\n",
    "        qkv_ = qkv(x).reshape(B, N, 3, num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv_.unbind(0)\n",
    "        q, k = q_norm(q), k_norm(k)\n",
    "        q = q * scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn + attn_mask if attn_mask is not None else attn\n",
    "        attn = attn.softmax(-1)\n",
    "        attn = attn_drop(attn)\n",
    "        x = epsilon_lrp(torch.matmul, 1e-6, attn.detach(), v)\n",
    "        x = x.transpose(1, 2).reshape(B,N,C)\n",
    "        x = norm(x)\n",
    "        x = proj(x)\n",
    "        x = proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c055c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module(module, input_layer=False):\n",
    "    if isinstance(module, torch.nn.Conv2d) and input_layer:\n",
    "        return conv_2d\n",
    "    elif isinstance(module, torch.nn.Dropout) or isinstance(module, torch.nn.Identity) or isinstance(module, torch.nn.GELU):\n",
    "        return identity\n",
    "    elif isinstance(module, torch.nn.LayerNorm) or isinstance(module, timm.layers.norm.LayerNorm):\n",
    "        return layernorm\n",
    "    elif isinstance(module, timm.layers.attention.Attention):\n",
    "        return multihead_attn_fn_cp\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        return linear\n",
    "    else:\n",
    "        return identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35a4bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_embed_block(input, block):\n",
    "    proj = get_module(block.proj)\n",
    "    x = proj(block.proj, input)\n",
    "    norm = get_module(block.norm)\n",
    "    x = norm(block.norm, x)\n",
    "    x = model._pos_embed(x.flatten(2).transpose(1,2))\n",
    "    return x\n",
    "def attn_block(input, block):\n",
    "    norm = get_module(block.norm1)\n",
    "    attn = get_module(block.attn)\n",
    "    ls1 = get_module(block.ls1)\n",
    "    drop_path1 = get_module(block.drop_path1)\n",
    "    norm2 = get_module(block.norm2)\n",
    "    fc1 = get_module(block.mlp.fc1)\n",
    "    act = get_module(block.mlp.act)\n",
    "    drop1 = get_module(block.mlp.drop1)\n",
    "    norm3 = get_module(block.mlp.norm)\n",
    "    fc2 = get_module(block.mlp.fc2)\n",
    "    drop2 = get_module(block.mlp.drop2)\n",
    "    ls2 = get_module(block.ls2)\n",
    "    drop_path2 = get_module(block.drop_path2)\n",
    "    x =add(input, drop_path1(inputs =ls1(inputs =attn(block.attn,norm(inputs = input, fn = block.norm1)), fn = block.ls1), fn = block.drop_path1))\n",
    "    x = add(x, drop_path2(inputs = ls2(inputs = drop2(inputs =fc2(block.mlp.fc2, norm3(inputs = drop1(inputs = act(inputs=fc1(block.mlp.fc1, norm2(inputs = x, fn = block.norm2)), fn = block.mlp.act), fn = block.mlp.drop1), fn = block.mlp.norm)), fn = block.mlp.drop2), fn = block.ls2), fn = block.drop_path2))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc8bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce61c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_children():\n",
    "    if isinstance(module, timm.layers.patch_embed.PatchEmbed):\n",
    "        y = patch_embed_block(x, module)\n",
    "    elif isinstance(module, torch.nn.Sequential):\n",
    "        for name, n_module in module.named_children():\n",
    "            if isinstance(n_module, timm.models.vision_transformer.Block):\n",
    "                y = attn_block(y, n_module)\n",
    "        \n",
    "    else:\n",
    "        if name == \"fc_norm\":\n",
    "            y = model.pool(y)\n",
    "        func = get_module(module)\n",
    "        y = func(inputs = y, fn = module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed78731",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = torch.zeros_like(y)\n",
    "R[0,torch.argmax(y)] = y[0,torch.argmax(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9c714e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 9.0226, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335041c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformers-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
